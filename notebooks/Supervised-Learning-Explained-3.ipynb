{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_photo(X):\n",
    "    \"\"\"\n",
    "    each pixel has range [0, 255]. Normalize it to [-1, 1]\n",
    "    \n",
    "    X:   Input photo\n",
    "    \"\"\"\n",
    "    return (X - 128.0 / 128.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initilize_parameters(layer_dims, activations):\n",
    "    \"\"\"\n",
    "    layer_dims: [n_x, n_1, n_2, ..., n_y]\n",
    "    activations: [not_used, relu, relu, ..., sigmoid]\n",
    "    \"\"\"\n",
    "    L = len(layer_dims) - 1\n",
    "    \n",
    "    W = [l for l in range(L + 1)] # W[0] - W[L], W[0] is not used\n",
    "    b = [l for l in range(L + 1)] # b[0] - b[L], b[0] is not used\n",
    "    \n",
    "    # Initialize parameters\n",
    "    for l in range(1, L + 1): # 1 - L\n",
    "        if activations[l] == \"relu\":        \n",
    "            norm = np.sqrt(2.0 / layer_dims[l-1]) # He Initialization, He et al., 2015\n",
    "        elif activations[l] == \"tanh\":\n",
    "            norm = 0.01\n",
    "        elif activations[l] == \"sigmoid\":\n",
    "            norm = 0.01\n",
    "        else:\n",
    "            norm = 1\n",
    "        if 'debug' in globals() and debug:            \n",
    "            print(\"layer\", l, \"[\", layer_dims[l], layer_dims[l-1], \"]\", activations[l], norm)\n",
    "            \n",
    "        W[l] = np.random.randn(layer_dims[l], layer_dims[l-1]) * norm # (n[l], n[l-1])\n",
    "        b[l] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 [ 2 4 ] relu 0.707106781187\n",
      "layer 2 [ 3 2 ] sigmoid 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, array([[ 1.14858562, -0.43257711, -0.37347383, -0.75870339],\n",
       "        [ 0.6119356 , -1.62743362,  1.23376823, -0.53825456]]), array([[ 0.00319039, -0.0024937 ],\n",
       "        [ 0.01462108, -0.02060141],\n",
       "        [-0.00322417, -0.00384054]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "debug=True\n",
    "W, b = initilize_parameters([4, 2, 3], [\"placeholder\", \"relu\", \"sigmoid\"])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, array([[ 0.],\n",
       "        [ 0.]]), array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b, activations, iter_i=-1):\n",
    "    \"\"\"\n",
    "    X: Input\n",
    "    W: [not_used, W1, W2, ..., WL]\n",
    "    b: [not_used, b1, b2, ..., bL]\n",
    "    activations: [not_used, relu, relu, ..., sigmoid]    \n",
    "    \"\"\"\n",
    "    L = len(W) - 1\n",
    "    \n",
    "    Z  = [l for l in range(L+1)]\n",
    "    A  = [l for l in range(L+1)]\n",
    "    \n",
    "    A[0] = X\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        Z[l] = np.dot(W[l], A[l - 1]) + b[l] # (n[l], m) <= (n[l], n[l-1]) . (n[l-1], m) + (n[l], 1)\n",
    "        \n",
    "        \n",
    "        A[l] = np.tanh(Z[l])\n",
    "        if activations[l] == \"relu\":        \n",
    "            A[l] = np.max(0, Z[l])\n",
    "        elif activations[l] == \"tanh\":\n",
    "            A[l] = np.tanh(Z[l])\n",
    "        elif activations[l] == \"sigmoid\":\n",
    "            A[l] = 1.0 / (1.0 + np.exp(-Z[l]))\n",
    "        else:\n",
    "            raise Exception(\"activation \" + activations[l] + \"not supported\")\n",
    "            \n",
    "        if iter_i == 0 and 'debug' in globals() and debug:\n",
    "            shape_info = \"Z[l] = np.dot(W[l], A[l - 1]) + b[l] {shape1} <= {shape2} . {shape3} + {shape4}\".format(\n",
    "                shape1=Z[l].shape, shape2=W[l].shape, shape3=A[l-1].shape, shape4=b[l].shape)\n",
    "            print(l, shape_info)\n",
    "            \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(X, A, Y, W, b, activations, iter_i=-1, lambd=0):\n",
    "    n_x, m = X.shape\n",
    "    L = len(W) - 1\n",
    "\n",
    "    dA = [l for l in range(L + 1)] # index 0 not used\n",
    "    dZ = [l for l in range(L + 1)]\n",
    "    dW = [l for l in range(L + 1)]\n",
    "    db = [l for l in range(L + 1)]\n",
    "\n",
    "    \n",
    "    # Backward propagation for the last layer\n",
    "    if activations[L] == \"sigmoid\":\n",
    "        dA[L] = -(np.divide(Y, A[L]) - np.divide(1 - Y, 1 - A[L]))\n",
    "        # The way to calculate dZL is different than other layers, due to different activation function\n",
    "        dZ[L] = A[L] - Y\n",
    "    else:\n",
    "        raise Exception(\"activation \" + activations[L] + \"not supported\")\n",
    "        \n",
    "    dW[L] = np.dot(dZ[L], A[L-1].T) / m  + lambd * W[L] / m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)\n",
    "\n",
    "    # Backward propagation for other layers\n",
    "    for l in reversed(range(1, L)):\n",
    "        if iter_i == 0 and 'debug' in globals() and debug:\n",
    "            print(l, \"W[l].shape\", W[l].shape, \"dZ[l+1].shape\", dZ[l+1])\n",
    "\n",
    "        if activations[l] == \"tanh\":\n",
    "            dgZl = 1 - np.power(A[l], 2)\n",
    "            dZ[l] = np.multiply(np.dot(W[l+1].T, dZ[l+1]), dgZl)\n",
    "#         elif activations[l] == \"relu\":\n",
    "        else:\n",
    "            raise Exception(\"activation \" + activations[L] + \"not supported\")\n",
    "            \n",
    "        dW[l] = np.dot(dZ[l], A[l-1].T) / m + lambd * W[l] / m\n",
    "        db[l] = np.sum(dZ[l], axis=1, keepdims=True)\n",
    "        \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cost(W, AL, Y, lambd=0):\n",
    "    n_y, m = Y.shape\n",
    "        \n",
    "    # calculate cost\n",
    "    \n",
    "    cross_entropy_cost = -(np.dot(Y, np.log(AL.T)) + np.dot(1-Y, np.log(1-AL.T))) / m\n",
    "    \n",
    "    regulation = 0\n",
    "    if lambd > 0:\n",
    "        for l in range(1, len(W)):\n",
    "            regulation += np.sum(W[l])\n",
    "            \n",
    "    return np.squeeze(cross_entropy_cost) + regulation / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(X, Y, test_set_x, test_set_y, hidden_layer_dims, activations, \n",
    "                   num_iterations=10, learning_rate=0.01, early_stop_cost=0., lambd=0):\n",
    "    \"\"\"\n",
    "    X:                 train input\n",
    "    Y:                 train labels\n",
    "    hidden_layer_dims: all hidden layer units\n",
    "    \"\"\"\n",
    "    n_x, m = X.shape\n",
    "    n_y, _ = Y.shape\n",
    "\n",
    "    # And input layer, and output layer\n",
    "    layer_dims = [n_x] + hidden_layer_dims + [n_y] \n",
    "    L = len(layer_dims) - 1\n",
    "\n",
    "    activations.insert(0, \"not_used\")\n",
    "        \n",
    "    W, b = initilize_parameters(layer_dims, activations)\n",
    "\n",
    "\n",
    "    \n",
    "    # print one param for debug\n",
    "    if 'debug' in globals() and debug:\n",
    "        print(\"init weights\", W[1][0][0], W[2][0][0])\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        A, Z = forward_propagation(X, W, b, activations)\n",
    "        dW, db = backward_propagation(X, A, Y, W, b, activations)\n",
    "            \n",
    "        if i == 0 and 'debug' in globals() and debug:\n",
    "            print(i, \" derivitive\", dW[1][0][0], dW[2][0][0])\n",
    "            print(i, \" derivitive\", db[1][0][0], db[2][0][0])\n",
    "\n",
    "        # update parameters\n",
    "        for l in range(1, L + 1):\n",
    "            W[l] = W[l] - learning_rate * dW[l]\n",
    "            b[l] = b[l] - learning_rate * db[l]\n",
    "            \n",
    "        if i == 0 and 'debug' in globals() and debug:        \n",
    "            print(i, \"weights\", W[1][0][0])\n",
    "\n",
    "        cost = calculate_cost(W, A[L], Y, lambd)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        if cost < early_stop_cost:\n",
    "            break\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            train_predict, train_accuracy, test_predict, test_accuracy = cat_utils.accuracy_n_layers(\n",
    "                W, b, test_set_x, test_set_y, A[L], train_set_y)\n",
    "                \n",
    "            print(\"epoch\", i, \"cost\", cost, \"train accuracy\", train_accuracy, \"test accuracy\", test_accuracy)\n",
    "\n",
    "    print(i, cost)\n",
    "    \n",
    "    return W, b, A, i, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 [ 1000 12288 ] tanh 0.01\n",
      "layer 2 [ 20 1000 ] tanh 0.01\n",
      "layer 3 [ 1 20 ] sigmoid 0.01\n",
      "init weights 0.0162434536366 0.00802711095375\n",
      "0  derivitive -7.87063390191e-06 -0.000868145506998\n",
      "0  derivitive -0.00651203787262 -0.352698735691\n",
      "0 weights 0.016243532343\n",
      "epoch 0 cost 0.760397016032 train accuracy 0.650717703349 test accuracy 0.34\n",
      "epoch 100 cost 0.710480814536 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 200 cost 0.709297703292 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 300 cost 0.70759692335 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 400 cost 0.704188997037 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 500 cost 0.696928691728 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 600 cost 0.682709396277 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 700 cost 0.658332287044 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 800 cost 0.620683752621 train accuracy 0.66028708134 test accuracy 0.4\n",
      "epoch 900 cost 0.566373932742 train accuracy 0.746411483254 test accuracy 0.56\n",
      "epoch 1000 cost 0.495380173977 train accuracy 0.818181818182 test accuracy 0.64\n",
      "epoch 1100 cost 0.417298271686 train accuracy 0.856459330144 test accuracy 0.8\n",
      "epoch 1200 cost 0.565461418712 train accuracy 0.722488038278 test accuracy 0.8\n",
      "epoch 1300 cost 0.544993456298 train accuracy 0.755980861244 test accuracy 0.8\n",
      "epoch 1400 cost 0.524446444637 train accuracy 0.784688995215 test accuracy 0.8\n",
      "epoch 1500 cost 0.501566271239 train accuracy 0.813397129187 test accuracy 0.8\n",
      "epoch 1600 cost 0.475016087189 train accuracy 0.832535885167 test accuracy 0.78\n",
      "epoch 1700 cost 0.442718992235 train accuracy 0.851674641148 test accuracy 0.78\n",
      "epoch 1800 cost 0.399968274749 train accuracy 0.88038277512 test accuracy 0.78\n",
      "epoch 1900 cost 0.188932109114 train accuracy 0.980861244019 test accuracy 0.76\n",
      "epoch 2000 cost 0.127054701524 train accuracy 0.99043062201 test accuracy 0.78\n",
      "epoch 2100 cost 0.106482859727 train accuracy 0.995215311005 test accuracy 0.76\n",
      "2111 0.0998423869055\n",
      "total training time 8718.566375732422\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cat_utils\n",
    "import numpy as np\n",
    "\n",
    "train_set_x, train_set_y, test_set_x, test_set_y, classes = cat_utils.load_normalized_dataset()\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(1)\n",
    "W, b, A, i, costs = neural_network(train_set_x, train_set_y, test_set_x, test_set_y, hidden_layer_dims=[1000, 20],\n",
    "                                activations=[\"tanh\", \"tanh\", \"sigmoid\"],\n",
    "                                num_iterations = 20001, learning_rate = 0.01, early_stop_cost=0.1, lambd=0.1)\n",
    "\n",
    "print(\"total training time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 [ 1000 12288 ] tanh 0.01\n",
      "layer 2 [ 20 1000 ] tanh 0.01\n",
      "layer 3 [ 1 20 ] sigmoid 0.01\n",
      "init weights 0.0162434536366 0.00802711095375\n",
      "0  derivitive -7.87063390191e-06 -0.000868145506998\n",
      "0  derivitive -0.00651203787262 -0.352698735691\n",
      "0 weights 0.016243532343\n",
      "epoch 0 cost 0.760397016032 train accuracy 0.650717703349 test accuracy 0.34\n",
      "epoch 100 cost 0.710480814536 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 200 cost 0.709297703292 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 300 cost 0.70759692335 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 400 cost 0.704188997037 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 500 cost 0.696928691728 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 600 cost 0.682709396277 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 700 cost 0.658332287044 train accuracy 0.655502392344 test accuracy 0.34\n",
      "epoch 800 cost 0.620683752621 train accuracy 0.66028708134 test accuracy 0.4\n",
      "epoch 900 cost 0.566373932742 train accuracy 0.746411483254 test accuracy 0.56\n",
      "epoch 1000 cost 0.495380173977 train accuracy 0.818181818182 test accuracy 0.64\n",
      "epoch 1100 cost 0.417298271686 train accuracy 0.856459330144 test accuracy 0.8\n",
      "epoch 1200 cost 0.565461418712 train accuracy 0.722488038278 test accuracy 0.8\n",
      "epoch 1300 cost 0.544993456298 train accuracy 0.755980861244 test accuracy 0.8\n",
      "epoch 1400 cost 0.524446444637 train accuracy 0.784688995215 test accuracy 0.8\n",
      "epoch 1500 cost 0.501566271239 train accuracy 0.813397129187 test accuracy 0.8\n",
      "epoch 1600 cost 0.475016087189 train accuracy 0.832535885167 test accuracy 0.78\n",
      "epoch 1700 cost 0.442718992235 train accuracy 0.851674641148 test accuracy 0.78\n",
      "epoch 1800 cost 0.399968274749 train accuracy 0.88038277512 test accuracy 0.78\n",
      "epoch 1900 cost 0.188932109114 train accuracy 0.980861244019 test accuracy 0.76\n",
      "epoch 2000 cost 0.127054701524 train accuracy 0.99043062201 test accuracy 0.78\n",
      "epoch 2100 cost 0.106482859727 train accuracy 0.995215311005 test accuracy 0.76\n",
      "2111 0.0998423869055\n",
      "total training time 807.2622032165527\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cat_utils\n",
    "import numpy as np\n",
    "\n",
    "train_set_x, train_set_y, test_set_x, test_set_y, classes = cat_utils.load_normalized_dataset()\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(1)\n",
    "W, b, A, i, costs = neural_network(train_set_x, train_set_y, test_set_x, test_set_y, hidden_layer_dims=[1000, 20],\n",
    "                                activations=[\"tanh\", \"tanh\", \"sigmoid\"],\n",
    "                                num_iterations = 20001, learning_rate = 0.01, early_stop_cost=0.1, lambd=0.3)\n",
    "\n",
    "print(\"total training time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.995215311005\n",
      "test accuracy 0.72\n"
     ]
    }
   ],
   "source": [
    "train_predict, train_accuracy, test_predict, test_accuracy = cat_utils.accuracy_n_layers(\n",
    "    W, b, test_set_x, test_set_y, A[len(W)-1], train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.885167464115\n",
      "test accuracy 0.54\n"
     ]
    }
   ],
   "source": [
    "train_predict, train_accuracy, test_predict, test_accuracy = cat_utils.accuracy_n_layers(\n",
    "    W, b, test_set_x, test_set_y, A[len(W)-1], train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rn=np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=np.random.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0338995437889325"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(0,27,3).reshape(3,3)\n",
    "\n",
    "result = a / np.linalg.norm(a, axis=-1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.4472136 ,  0.89442719],\n",
       "       [ 0.42426407,  0.56568542,  0.70710678],\n",
       "       [ 0.49153915,  0.57346234,  0.65538554]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  6],\n",
       "       [ 9, 12, 15],\n",
       "       [18, 21, 24]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.70820393,  21.21320344,  36.61966685])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.power(a[:, 0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.848570571257099"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06666667,  0.76862745,  0.32156863, ...,  0.56078431,\n",
       "         0.08627451,  0.03137255],\n",
       "       [ 0.12156863,  0.75294118,  0.27843137, ...,  0.60784314,\n",
       "         0.09411765,  0.10980392],\n",
       "       [ 0.21960784,  0.74509804,  0.26666667, ...,  0.64705882,\n",
       "         0.09019608,  0.20784314],\n",
       "       ..., \n",
       "       [ 0.        ,  0.32156863,  0.54117647, ...,  0.33333333,\n",
       "         0.01568627,  0.        ],\n",
       "       [ 0.        ,  0.31372549,  0.55294118, ...,  0.41960784,\n",
       "         0.01960784,  0.        ],\n",
       "       [ 0.        ,  0.31764706,  0.55686275, ...,  0.58431373,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = cat_utils.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 31, 56], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_orig[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_set_x_orig, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(train_set_x_orig, axis=0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = np.sum(np.power(train_set_x_orig, 2), axis=0) / train_set_x_orig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normed = (train_set_x_orig - mean)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58989828,  0.16329855,  0.40641656])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed[1][10][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
